{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add credits and adjust the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the diffusion transformer itself\n",
    "\n",
    "class DiT(tf.keras.models.Model):\n",
    "    def __init__(self, img_size, patch_size, model_dim=256, k=64, heads=4, mlp_dim=512, depth=3, cuant_dim=4):\n",
    "        super(DiT, self).__init__()\n",
    "        \n",
    "        # size of patches we decompose images to\n",
    "        # also calculate the number of patches we get\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size//patch_size)**2\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.patches = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(model_dim, kernel_size=patch_size, strides=patch_size, padding='same'),\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.pos = PositionalEmbedding(self.n_patches, model_dim)\n",
    "        self.sin_emb = TimestepEmbedder(model_dim)\n",
    "        self.transformer = [DiTBlock(model_dim,\n",
    "                            heads, mlp_dim, mod_init='zeros', k=k) for _ in range(depth)]\n",
    "        self.final_layer = FinalLayer(patch_size, cuant_dim,\n",
    "                                     initializer='zeros')\n",
    "    \n",
    "    def call(self, x):\n",
    "        # this gets the latent input x from the encoder\n",
    "        noisy_latent, noise_variances = x\n",
    "        B = noise_variances.shape[0]\n",
    "\n",
    "        # adding the batch dimension to the noise variances\n",
    "        noise_variances = tf.reshape(noise_variances, [B, -1])\n",
    "\n",
    "        # using the sine embedding to get the time embedding\n",
    "        t = self.sin_emb(noisy_latent)\n",
    "\n",
    "        # splitting x into the patches\n",
    "        x = self.patches(noisy_latent)\n",
    "\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        x = tf.reshape(x, [B, H*W, C])\n",
    "\n",
    "        x = self.pos(x)\n",
    "\n",
    "        # forward pass through the DiT\n",
    "        for i in range(self.depth):\n",
    "            x = self.transformer[i](x, t)\n",
    "\n",
    "        # first, adaLn (adaptive layer norm for the layer norm)\n",
    "        # then linear layer\n",
    "        x = self.final_layer(x, t)\n",
    "\n",
    "        # final reshape\n",
    "        x = tf.reshape(x, [B, H, W, -1])\n",
    "\n",
    "        # we have to do this because we were using patches\n",
    "        x = tf.nn.depth_to_space(x, self.patch_size, data_format='NHWC')\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiTBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dim, n_heads=2, mlp_dim=512, rate=0.0, eps=1e-6, \n",
    "                 initializer='glorot_uniform', mod_init='glorot_uniform', k=64, **kwargs):\n",
    "        super(DiTBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.attn = LinformerAttention(model_dim, n_heads, k=k, initializer=initializer)\n",
    "\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_dim, activation='gelu', kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(model_dim, kernel_initializer=initializer),\n",
    "        ])\n",
    "\n",
    "        self.sm1 = AdaLN(epsilon=eps, initializer=mod_init)\n",
    "        self.sm2 = AdaLN(epsilon=eps, initializer=mod_init)\n",
    "        self.scale1 = Scale(initializer=mod_init)\n",
    "        self.scale2 = Scale(initializer=mod_init)\n",
    "\n",
    "    def call(self, inputs, z, training):\n",
    "        # TODO: what about the conditioning??\n",
    "        # so the conditioning gives us the z (this is the time embedding)\n",
    "        # this is what we use for the alpha, gamma, and beta parameters later\n",
    "\n",
    "\n",
    "        # first layer norm\n",
    "        # this will do the scale and shift also\n",
    "        x = self.sm1(inputs, z)\n",
    "\n",
    "        # attention\n",
    "        x = self.attn(x, x, x)\n",
    "\n",
    "        # scale and shift\n",
    "        x = self.scale1(x, z)\n",
    "\n",
    "        # first residual\n",
    "        out1 = x + inputs\n",
    "\n",
    "        # second layer norm\n",
    "        # this will do the scale and shift also\n",
    "        x = self.sm2(out1, z)\n",
    "\n",
    "        # mlp input\n",
    "        x = self.mlp(x) # this is the pointwise feedforward\n",
    "\n",
    "        # scale and shift\n",
    "        out2 = self.scale2(x, z)\n",
    "\n",
    "        # add the residual here\n",
    "        return out1 + out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  scale - learn the alpha parameter\n",
    "\n",
    "class Scale(tf.keras.layers.Layer):\n",
    "    def __init__(self, initializer='glorot_uniform', **kwargs):\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "        self.initializer = initializer\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # use_bias just means it uses a bias vector (set to True by default)\n",
    "        # input_shape[2] is number of output nodes -> third element of the input shape\n",
    "        # kernel_initializer is just initializing the weights\n",
    "\n",
    "        # TODO: understand the input shape\n",
    "        self.alpha = tf.keras.layers.Dense(input_shape[2], use_bias=True, kernel_initializer=self.initializer)\n",
    "    \n",
    "    def call(self, x, z):\n",
    "        scale = self.alpha(z)\n",
    "        x *= tf.expand_dims(scale, axis=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_patches, model_dim, initializer='glorot_uniform', **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_patches = n_patches\n",
    "\n",
    "        # TODO: it uses glorot uniform already, you don't need to add it\n",
    "        self.positional_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=n_patches, output_dim=model_dim, embeddings_initializer=initializer\n",
    "        )\n",
    "\n",
    "    def call(self, patches):\n",
    "        # delta is the spacing\n",
    "        positions = tf.range(start=0, limit=self.n_patches, delta=1)\n",
    "\n",
    "        # we apply the positional embedding to the input\n",
    "        return patches + self.positional_embedding(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepEmbedder(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dim, initializer='glorot_uniform', **kwargs):\n",
    "        super(TimestepEmbedder, self).__init__(**kwargs)\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            # model_dim is the output dimension\n",
    "            tf.keras.layers.Dense(model_dim, activation='silu', kernel_initializer=initializer),\n",
    "            # no activation here (linear activation)\n",
    "            tf.keras.layers.Dense(model_dim, kernel_initializer=initializer),\n",
    "        ])\n",
    "\n",
    "    def sinusoidal_embedding(self, x):\n",
    "        embedding_min_freq = 1.0\n",
    "        noise_embedding_max_freq = 1000.0\n",
    "\n",
    "        frequencies = tf.exp(\n",
    "            tf.linspace(\n",
    "                tf.math.log(embedding_min_freq),\n",
    "                tf.math.log(noise_embedding_max_freq),\n",
    "                self.model_dim//2\n",
    "            )\n",
    "        )\n",
    "\n",
    "        angular_speeds = 2.0 * math.pi * frequencies\n",
    "\n",
    "        embeddings = tf.concat(\n",
    "            [\n",
    "                tf.sin(angular_speeds * x),\n",
    "                tf.cos(angular_speeds * x)\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return embeddings\n",
    "    def call(self, x):\n",
    "        x = tf.keras.layers.Lambda(self.sinusoidal_embedding)(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer where we apply layer norm and then linear\n",
    "class FinalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size, out_channels, eps=1e-6, initializer='glorot_uniform', **kwargs):\n",
    "        super(FinalLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.linear = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(patch_size*patch_size*out_channels, kernel_initializer=initializer),\n",
    "        ])\n",
    "\n",
    "        self.sm = AdaLN(epsilon=eps, initializer=initializer)\n",
    "\n",
    "    def call(self, inputs, z, training):\n",
    "        x = self.sm(inputs, z)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the AdaLN linear norm layer\n",
    "\n",
    "class AdaLN(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-3, initializer='glorot_uniform'):\n",
    "        super(AdaLN, self).__init__()\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.initializer = initializer\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=epsilon, center=False, scale=False)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # getting the gamma and beta parameters here\n",
    "\n",
    "        # z is 32x32x4\n",
    "        # so input_shape[2] is 4 (one for each block)\n",
    "        # and dense layer would output (batch_size, 4)\n",
    "    \n",
    "\n",
    "        self.gamma = tf.keras.layers.Dense(input_shape[2], use_bias=True, kernel_initializer=self.initializer)\n",
    "        self.beta = tf.keras.layers.Dense(input_shape[2], use_bias=True, kernel_initializer=self.initializer)\n",
    "\n",
    "\n",
    "    def call(self, x, z):\n",
    "        # getting the gamma and beta parameters\n",
    "        gamma = self.gamma(z)\n",
    "        beta = self.beta(z)\n",
    "\n",
    "        # gamma is the scale\n",
    "        # beta is the shift\n",
    "\n",
    "        # normalizing the input\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # applying the gamma and beta parameters\n",
    "\n",
    "        # do 1 + so that it doesn't just scale to 0 if gamma is 0\n",
    "        # expand dim adds 1 dimension to end (just to make shapes work out and we can broadcast)\n",
    "        x = x * (1+tf.expand_dims(gamma, axis=1)) + tf.expand_dims(beta, axis=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, time to implement the Linformer attention\n",
    "\n",
    "class LinformerAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dim, n_heads, k, rate=0.0, initializer='glorot_uniform', **kwargs):\n",
    "        super(LinformerAttention, self).__init__(**kwargs)\n",
    "        self.n_heads = n_heads\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "\n",
    "        assert model_dim % self.n_heads == 0\n",
    "\n",
    "        # dimension of each head\n",
    "        self.head_dim = model_dim // self.n_heads\n",
    "\n",
    "        # weights for the query, key, and value\n",
    "        self.wq = tf.keras.layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        self.wk = tf.keras.layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        self.wv = tf.keras.layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "\n",
    "        # now, we use E and F to save computational resources\n",
    "        \n",
    "        self.E = tf.keras.layers.Dense(k)\n",
    "        self.F = tf.keras.layers.Dense(k)\n",
    "\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # self.w0 - how we concatenate the heads\n",
    "        self.w0 = tf.keras.layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "\n",
    "        # -1 tells tensorflow to figure out the size of the dimension\n",
    "        # -1 depends on the size of the input tensor\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    # we just send in x, x, x because we are using the same input for q, k, and v\n",
    "    def call(self, q, k, v):\n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "\n",
    "        # so we can do multihead attention\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "\n",
    "        # TODO: why do we split afterthe matrix multiply?\n",
    "\n",
    "        # getting dimensionality\n",
    "        dh = tf.cast(self.head_dim, tf.float32)\n",
    "\n",
    "        # getting qk\n",
    "        qk = tf.matmul(q, k, transpose=True)\n",
    "        scaled_qk = qk / tf.math.sqrt(dh)\n",
    "\n",
    "        attn = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        attn = self.dropout1(attn)\n",
    "        attn = tf.matmul(attn, v)\n",
    "\n",
    "        # og attention\n",
    "        # we undo the split heads by transposing\n",
    "        attn = tf.transpose(attn, perm=[0, 2, 1, 3])\n",
    "        original_size_attention = tf.reshape(attn, (batch_size, -1, self.model_dim))\n",
    "\n",
    "        # now that we have the attention, we concatenate the heads\n",
    "        # have to get og size with w0\n",
    "\n",
    "        out = self.w0(original_size_attention)\n",
    "        out = self.dropout2(self.w0(original_size_attention))\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
