{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionTransformer(tf.keras.models.Model):\n",
    "    def __init__(self, position_embedder, time_embedder, dit_block, patcher, adaln, linear_layer):\n",
    "        super(LatentDiffusionTransformer, self).__init__()\n",
    "\n",
    "\n",
    "        # ViT sine-cosine frequency embedder\n",
    "        self.position_embedder = position_embedder\n",
    "\n",
    "        # time embedder\n",
    "        # 256-dimensional frequency embedding\n",
    "        # followed by two-layer MLP with SiLU activation\n",
    "        self.time_embedder = time_embedder\n",
    "\n",
    "        # linearly embeds patch in input\n",
    "        # creates tokens T, determined by patch hyperparameter p\n",
    "        self.patchify = patcher\n",
    "\n",
    "        # entire block, including transformer\n",
    "        self.dit_block = dit_block\n",
    "\n",
    "        # feeds sum of timestep and class embeddings\n",
    "        # into SiLU nonlinearity and linear layer\n",
    "        self.ada_layernorm = adaln\n",
    "        self.linear = linear_layer\n",
    "\n",
    "    # forward pass through the diffusion transformer\n",
    "    def call(self, x):\n",
    "\n",
    "        # x is the latent input from the encoder\n",
    "\n",
    "        noised_latent = x\n",
    "\n",
    "        # obtaining the timestep embedding\n",
    "        t = self.time_embedder(x)\n",
    "\n",
    "        # split x into patches\n",
    "        x = self.patchify(noised_latent)\n",
    "\n",
    "        # obtaining information for reshape\n",
    "        b, h, w, c = x.shape\n",
    "\n",
    "        # obtain the positional embedding\n",
    "        # this is a ViT frequency-based sine-cosine embedding \n",
    "        # that is added to the patch embeddings\n",
    "        pos = self.position_embedder(x)\n",
    "        x = x + pos\n",
    "\n",
    "        # pass through the DiT Block with adaLN-zero\n",
    "        x = self.dit_block(x, t)\n",
    "\n",
    "        # layer norm\n",
    "        x = self.ada_layernorm(x, t)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # linear reshape\n",
    "        x = tf.reshape(x, [b, h, w, -1])\n",
    "\n",
    "        # combine after patches\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiTBlock(tf.keras.models.Model):\n",
    "    def __init__(self, attention, adaln, scale, mlp, sands):\n",
    "        super(DiTBlock, self).__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "\n",
    "        self.shiftandscale = sands\n",
    "\n",
    "        self.ada_layernorm = adaln\n",
    "\n",
    "        self.adaln_1 = adaln\n",
    "        self.scale_1 = scale\n",
    "\n",
    "\n",
    "        self.adaln_2 = adaln\n",
    "        self.scale_2 = scale\n",
    "\n",
    "        # two layer feedforward (dense layers)\n",
    "        self.feedforward = mlp\n",
    "\n",
    "    def call(self, x, t):\n",
    "        # x and t are the input tokens\n",
    "        input_x = x\n",
    "\n",
    "        # first layer norm\n",
    "        x, omega, beta = self.adaln_1(x, t)\n",
    "\n",
    "        # scale and shift\n",
    "        x = self.shiftandscale(x, omega, beta)\n",
    "\n",
    "        # apply multihead attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # apply scale\n",
    "        x = self.scale_1(x)\n",
    "\n",
    "        # first residual addition\n",
    "        first_out = x + input_x\n",
    "\n",
    "\n",
    "        ## Now, second half of the DiT block\n",
    "        x, omega, beta = self.adaln_2(first_out, t)\n",
    "\n",
    "        # scale and shift\n",
    "        x = self.shiftandscale(x, omega, beta)\n",
    "\n",
    "        # feedforward\n",
    "        x = self.feedforward(x)\n",
    "\n",
    "        # scale\n",
    "        x = self.scale_2(x, t)\n",
    "\n",
    "        # final residual addition\n",
    "        out = first_out + x \n",
    "\n",
    "        return out "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
